{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is to create the Training Data Set, which is a subset of PREVIOUS and NEW datasets\n",
    "#### Note:  All of below can be run locally (using mostly pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"storage/sample/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the PREVIOUS corpus data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(810165, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_PREV = pd.read_csv(f'{PATH}data_PREV.csv')\n",
    "df_PREV.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the NEW corpus data file\n",
    "#### This will be used to compare against the PREVIOUS data file, and ensure inclusion in the new training set later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(865516, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NEW = pd.read_csv(f'{PATH}data_NEW.csv')\n",
    "df_NEW.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Label Frequency counts for each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbl_counts_P = df_PREV.groupby(['brand_id']).count().sort_values(by='text', ascending=False)\n",
    "df_lbl_counts_N = df_NEW.groupby(['brand_id']).count().sort_values(by='text', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREVIOUS Unique Label counts: 26006\n",
      "NEW Unique Label counts: 27328\n",
      "Difference: 1322\n"
     ]
    }
   ],
   "source": [
    "p, n = df_lbl_counts_P.shape[0], df_lbl_counts_N.shape[0]\n",
    "print('PREVIOUS Unique Label counts: ' + str(p))\n",
    "print('NEW Unique Label counts: ' + str(n))\n",
    "print('Difference: ' + str(n-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the PREVIOUS Record counts and remove records with low label frequency counts (< FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREVIOUS Record counts: 810165\n",
      "PREVIOUS Record counts (Adjusted): 802557\n",
      "Difference after Removing: -7608\n"
     ]
    }
   ],
   "source": [
    "# Let's only consider labels that have occurred at least FREQ times:\n",
    "FREQ = 2\n",
    "\n",
    "items_to_drop = df_lbl_counts_P[df_lbl_counts_P.text < FREQ].index\n",
    "\n",
    "# return rows that do NOT (~) include items_to_drop\n",
    "df_P2 = df_PREV[~df_PREV['brand_id'].isin(items_to_drop)]\n",
    "\n",
    "p, pa = df_PREV.count()[0], df_P2.count()[0]\n",
    "\n",
    "print('PREVIOUS Record counts: ' + str(p))\n",
    "print('PREVIOUS Record counts (Adjusted): ' + str(pa))\n",
    "print('Difference after Removing: ' + str(pa-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the NEW Record counts and remove records with low label frequency counts (< FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW Record counts: 865516\n",
      "NEW Record counts (Adjusted): 857559\n",
      "Difference after Removing: -7957\n"
     ]
    }
   ],
   "source": [
    "# Do same for NEW Dataset\n",
    "items_to_drop = df_lbl_counts_N[df_lbl_counts_N.text < FREQ].index\n",
    "\n",
    "# return rows that do NOT (~) include items_to_drop\n",
    "df_N2 = df_NEW[~df_NEW['brand_id'].isin(items_to_drop)]\n",
    "\n",
    "n,na = df_NEW.count()[0], df_N2.count()[0]\n",
    "\n",
    "print('NEW Record counts: ' + str(n))\n",
    "print('NEW Record counts (Adjusted): ' + str(na))\n",
    "print('Difference after Removing: ' + str(na-n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the (Unique) Label Frequency counts for each of the ADJUSTED datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lbl_counts_P = df_P2.groupby(['brand_id']).count().sort_values(by='text', ascending=False)\n",
    "df_lbl_counts_N = df_N2.groupby(['brand_id']).count().sort_values(by='text', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREVIOUS Unique Label counts: 18398\n",
      "NEW Unique Label counts: 19371\n",
      "Difference: 973\n"
     ]
    }
   ],
   "source": [
    "pp, nn = df_lbl_counts_P.shape[0], df_lbl_counts_N.shape[0]\n",
    "print('PREVIOUS Unique Label counts: ' + str(pp))\n",
    "print('NEW Unique Label counts: ' + str(nn))\n",
    "print('Difference: ' + str(nn-pp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the count of NEW Unique Labels that weren't there in the PREVIOUS dataset\n",
    "#### We will make sure to include these new labels in our new training file later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1056 unique labels in NEW Dataset that are NOT found in PREV dataset.\n"
     ]
    }
   ],
   "source": [
    "# Compare labels in PREV dataset to labels in NEW dataset\n",
    "p = set(df_lbl_counts_P.index)\n",
    "n = set(df_lbl_counts_N.index)\n",
    "if (n.issubset(p) == False) or (p.issuperset(n) == False):\n",
    "    labels_unique = sorted(n.difference(p))\n",
    "    print(\"There are \" + str(len(labels_unique)) + \" unique labels in NEW Dataset that are NOT found in PREV dataset.\")\n",
    "else:\n",
    "    print(\"No unique labels in New Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep aside all records with NEW lables in the new training set\n",
    "#### These we want as Must-Take in our new training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9079, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_must_take_TEMP = df_N2[df_N2['brand_id'].isin(labels_unique)]\n",
    "df_must_take_TEMP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a sample of the Must-Take (New Labels) data and put it aside for the Holdout (Test) Dataset\n",
    "#### (Since most of the classification predictions will probably come from this new label set in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOLDOUT_SIZE = 500\n",
    "\n",
    "df_tst = df_must_take_TEMP.sample(HOLDOUT_SIZE, random_state = 42)\n",
    "df_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the holdout test data out to a file to be used another time\n",
    "df_tst.to_csv(f'{PATH}holdout_for_Sampling_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8579, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop those holdout records from the training dataset\n",
    "df_must_take = df_must_take_TEMP.drop(index=df_tst.index)\n",
    "df_must_take.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which records are remaining (labels found in BOTH New and Previous datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(848480, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_remaining = df_N2[~df_N2['brand_id'].isin(labels_unique)]\n",
    "df_remaining.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TR_SIZE is desired size of Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16421"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TR_SIZE = 25000\n",
    "\n",
    "# Must include the Must-Have labels (New to this dataset;  the remainder can be appended for training)\n",
    "df_remaining = df_remaining.sample(TR_SIZE - (df_must_take.count()[0]), random_state = 42)\n",
    "df_remaining.count()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the new Sample dataset we will use for training\n",
    "#### Append the Remaining with the Must-Have (New unique labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ts = df_must_take.append(df_remaining)\n",
    "df_ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the training data out to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts.to_csv(f'{PATH}training_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
